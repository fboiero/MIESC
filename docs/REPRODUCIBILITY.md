# Reproducibilidad CientÃ­fica - MIESC

**Autor:** Fernando Boiero
**InstituciÃ³n:** Universidad de la Defensa Nacional (UNDEF) - IUA CÃ³rdoba
**Programa:** MaestrÃ­a en Ciberdefensa
**Fecha:** Enero 2025

---

## ğŸ¯ Objetivo

Este documento proporciona instrucciones completas para reproducir todos los experimentos y resultados presentados en la tesis "Integrated Security Assessment Framework for Smart Contracts: A Defense-in-Depth Approach to Cyberdefense".

---

## ğŸ“‹ Requisitos Previos

### Hardware MÃ­nimo Recomendado
- **CPU:** 4 cores (8 threads recomendado)
- **RAM:** 16 GB (32 GB recomendado para experimentos completos)
- **Almacenamiento:** 50 GB libres
- **Sistema Operativo:** Linux (Ubuntu 22.04 LTS), macOS 12+, o Windows 11 con WSL2

### Software Requerido

```bash
# Python 3.9+
python --version  # Debe ser >= 3.9.0

# Git
git --version

# Herramientas de seguridad
pip install slither-analyzer mythril
cargo install aderyn  # Opcional pero recomendado

# Node.js (para Solhint)
node --version  # >= 16.0.0
npm install -g solhint
```

---

## ğŸ”§ ConfiguraciÃ³n del Entorno

### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/fboiero/MIESC.git
cd MIESC
git checkout v3.0.0  # Usar versiÃ³n especÃ­fica de la tesis
```

### Paso 2: Crear Entorno Virtual

```bash
python -m venv venv

# Linux/macOS
source venv/bin/activate

# Windows
venv\Scripts\activate
```

### Paso 3: Instalar Dependencias

```bash
# InstalaciÃ³n completa
make install

# O manualmente:
pip install -r requirements.txt
pip install -r requirements_core.txt
pip install -r requirements_agents.txt
```

### Paso 4: Verificar InstalaciÃ³n

```bash
make verify
```

**Salida esperada:**
```
âœ“ Python 3.9+ detected
âœ“ Slither installed
âœ“ Mythril installed
âœ“ MIESC CLI functional
```

---

## ğŸ“Š ReproducciÃ³n de Experimentos

### Experimento 1: ConfiguraciÃ³n de Datasets

```bash
# Descargar y preparar datasets
python analysis/experiments/00_setup_experiments.py
```

Este script:
1. Crea estructura de directorios
2. Descarga SmartBugs curated dataset (143 contratos)
3. Genera ground truth labels
4. Crea configuraciÃ³n experimental

**Salida esperada:**
- `analysis/experiments/datasets/` - Contratos de prueba
- `analysis/experiments/ground_truth/` - Etiquetas verificadas
- `analysis/experiments/results/experiment_config.json`

### Experimento 2: EjecuciÃ³n de AuditorÃ­as

```bash
# Ejecutar suite completa de experimentos
python analysis/experiments/10_run_experiments.py

# O usar Makefile:
make experiments-run
```

**ParÃ¡metros experimentales:**
- **Grupos de control:** Slither solo, Mythril solo
- **Grupo experimental:** MIESC con correlaciÃ³n AI
- **MÃ©tricas calculadas:** Precision, Recall, F1, Cohen's Îº
- **Repeticiones:** 3 por contrato para estabilidad estadÃ­stica

**Tiempo estimado:** 2-4 horas (depende del hardware)

### Experimento 3: AnÃ¡lisis EstadÃ­stico

```bash
# Analizar resultados y calcular mÃ©tricas
python analysis/experiments/20_analyze_results.py
```

**MÃ©tricas generadas:**
```json
{
  "miesc_full": {
    "precision": 0.8947,
    "recall": 0.862,
    "f1_score": 0.8781,
    "cohens_kappa": 0.847
  },
  "baseline_slither": {
    "precision": 0.673,
    "recall": 0.941,
    "f1_score": 0.785
  },
  "baseline_mythril": {
    "precision": 0.728,
    "recall": 0.685,
    "f1_score": 0.706
  }
}
```

### Experimento 4: GeneraciÃ³n de GrÃ¡ficos

```bash
# Generar visualizaciones para la tesis
python analysis/experiments/30_generate_plots.py
```

**GrÃ¡ficos generados:**
- Figura 4.1: ComparaciÃ³n de Precision/Recall/F1
- Figura 4.2: Matriz de confusiÃ³n
- Figura 4.3: DistribuciÃ³n de tiempos de ejecuciÃ³n
- Figura 4.4: ReducciÃ³n de falsos positivos con AI

---

## ğŸ”¬ ValidaciÃ³n de Resultados

### Verificar MÃ©tricas Clave

```bash
# Calcular mÃ©tricas del paper
python src/miesc_cli.py metrics \
  analysis/experiments/results/predictions_miesc.json \
  analysis/experiments/ground_truth/binary_labels.json
```

**Resultado esperado:**
```
Precision:     0.8947  (Â±0.02)
Recall:        0.8620  (Â±0.03)
F1 Score:      0.8781  (Â±0.02)
Cohen's Kappa: 0.8470  (Â±0.04)

InterpretaciÃ³n:
âœ“ Precision > 0.89 â†’ 9 de cada 10 hallazgos son verdaderos positivos
âœ“ Recall > 0.86 â†’ Detecta 86% de todas las vulnerabilidades
âœ“ Cohen's Îº > 0.84 â†’ Acuerdo "casi perfecto" con expertos
```

### ComparaciÃ³n con Estado del Arte

```python
# Comparar con resultados publicados (Durieux et al., 2020)
import json

with open('analysis/experiments/results/comparison.json', 'r') as f:
    comparison = json.load(f)

for tool, metrics in comparison['tools'].items():
    print(f"{tool}: F1={metrics['f1_score']:.4f}")

# Salida esperada:
# MIESC: F1=0.8781  â† Nuestro framework
# Slither: F1=0.7850
# Mythril: F1=0.7060
# Mejora: +11.9% sobre mejor herramienta individual
```

---

## ğŸ“ Pruebas de Validez

### Validez Interna

**Control de variables confusoras:**

```bash
# Misma versiÃ³n de Solidity para todos los tests
export SOLC_VERSION=0.8.20

# Mismo timeout para todas las herramientas
export TOOL_TIMEOUT=300

# EjecuciÃ³n en entorno controlado (Docker)
docker run --rm -v $(pwd):/workspace miesc:latest \
  python analysis/experiments/10_run_experiments.py
```

### Validez Externa

**GeneralizaciÃ³n a contratos reales:**

```bash
# Test con contratos de producciÃ³n verificados en Etherscan
python analysis/experiments/40_validate_etherscan.py \
  --sample-size 100 \
  --category defi
```

### Validez de Constructo

**VerificaciÃ³n de ground truth:**

```bash
# Comparar anotaciones de 3 expertos independientes
python analysis/experiments/50_inter_rater_agreement.py
```

**Resultado esperado:** Cohen's Îº entre anotadores > 0.80

---

## ğŸ” ReplicaciÃ³n Paso a Paso

### ReplicaciÃ³n MÃ­nima (30 minutos)

```bash
# Dataset reducido para verificaciÃ³n rÃ¡pida
make test-quick
python src/miesc_cli.py run-audit \
  analysis/experiments/datasets/sample/reentrancy_vulnerable.sol \
  --enable-ai -o results_replication.json
```

### ReplicaciÃ³n Parcial (2-3 horas)

```bash
# 100 contratos del dataset SmartBugs
python analysis/experiments/10_run_experiments.py --sample 100
make experiments-analyze
```

### ReplicaciÃ³n Completa (1-2 dÃ­as)

```bash
# Todos los 5,127 contratos
python analysis/experiments/10_run_experiments.py --full
make experiments-analyze
make reproducibility  # Generar paquete de reproducibilidad
```

---

## ğŸ“¦ Paquete de Reproducibilidad

### Contenido del Paquete

```
thesis/reproducibility_YYYYMMDD.tar.gz
â”œâ”€â”€ datasets/               # Todos los contratos de prueba
â”œâ”€â”€ ground_truth/          # Etiquetas verificadas
â”œâ”€â”€ experiment_scripts/    # Scripts de experimentos
â”œâ”€â”€ raw_results/          # Resultados sin procesar
â”œâ”€â”€ processed_results/    # MÃ©tricas calculadas
â”œâ”€â”€ plots/                # GrÃ¡ficos generados
â”œâ”€â”€ REPRODUCIBILITY.md    # Este documento
â””â”€â”€ checksums.txt         # Hashes SHA256 de archivos
```

### Generar Paquete

```bash
make reproducibility
```

### Verificar Integridad

```bash
cd thesis/
tar -xzf reproducibility_YYYYMMDD.tar.gz
cd reproducibility/
sha256sum -c checksums.txt
```

---

## ğŸ› SoluciÃ³n de Problemas

### Error: "Tool X not found"

```bash
# Verificar PATH
which slither
which myth

# Reinstalar herramientas
pip install --upgrade slither-analyzer mythril
```

### Error: "Out of memory"

```bash
# Reducir carga de trabajo
export MAX_PARALLEL_TOOLS=2
python analysis/experiments/10_run_experiments.py --sample 50
```

### Error: "Timeout en Mythril"

```bash
# Aumentar timeout
export TOOL_TIMEOUT=600
make experiments-run
```

### Resultados Difieren Ligeramente

**Causas esperadas:**
- **Versiones de herramientas:** Actualizar a versiones especÃ­ficas
- **LLM non-determinism:** Fijar `temperature=0` en config
- **Variabilidad estadÃ­stica:** Ejecutar mÃºltiples repeticiones

**Tolerancia aceptable:**
- Precision: Â±2%
- Recall: Â±3%
- F1: Â±2%
- Cohen's Îº: Â±4%

---

## ğŸ“§ Soporte

Si encuentra problemas durante la reproducciÃ³n:

1. **Revisar Issues:** https://github.com/fboiero/MIESC/issues
2. **DocumentaciÃ³n:** https://github.com/fboiero/MIESC/docs
3. **Email:** fboiero@frvm.utn.edu.ar

---

## ğŸ“œ Licencia de Datos

- **CÃ³digo:** GPL-3.0
- **Datasets:** CC BY 4.0
- **Thesis:** CC BY-NC-ND 4.0

---

## âœ… Checklist de Reproducibilidad

- [ ] Entorno configurado correctamente (`make verify` pasa)
- [ ] Datasets descargados (`00_setup_experiments.py` ejecutado)
- [ ] Experimentos ejecutados (`10_run_experiments.py` completado)
- [ ] MÃ©tricas calculadas (resultados en `analysis/experiments/results/`)
- [ ] GrÃ¡ficos generados (figuras en `thesis/figures/`)
- [ ] Resultados validados (mÃ©tricas dentro de tolerancia)
- [ ] Paquete de reproducibilidad creado

**Firma digital (SHA256 del paquete de reproducibilidad):**
```
[SerÃ¡ generado al ejecutar make reproducibility]
```

---

**Ãšltima actualizaciÃ³n:** 2025-01-01
**VersiÃ³n MIESC:** 3.0.0
**Autor:** Fernando Boiero - UNDEF
