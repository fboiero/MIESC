# Ollama Models Configuration for MIESC
#
# This file defines available Ollama models and their characteristics
#
# Installation:
#   ollama pull <model_name>
#
# Usage:
#   python xaudit.py --target contract.sol --ai-model codellama:13b

models:
  # Code-focused models
  codellama:13b:
    description: "Meta's CodeLlama 13B - Best for code analysis"
    size: "7.4 GB"
    parameters: "13B"
    use_case: "general_code_analysis"
    performance:
      speed: "medium"
      quality: "high"
      memory: "16GB RAM recommended"
    recommended_for:
      - "Solidity analysis"
      - "Vulnerability detection"
      - "Code explanation"
    temperature: 0.1
    max_tokens: 2000

  codellama:7b:
    description: "Meta's CodeLlama 7B - Faster, smaller version"
    size: "3.8 GB"
    parameters: "7B"
    use_case: "fast_code_analysis"
    performance:
      speed: "fast"
      quality: "good"
      memory: "8GB RAM recommended"
    recommended_for:
      - "Quick analysis"
      - "CI/CD pipelines"
      - "Development"
    temperature: 0.1
    max_tokens: 1500

  deepseek-coder:6.7b:
    description: "DeepSeek Coder 6.7B - Specialized code model"
    size: "3.8 GB"
    parameters: "6.7B"
    use_case: "specialized_code"
    performance:
      speed: "fast"
      quality: "high"
      memory: "8GB RAM recommended"
    recommended_for:
      - "Code understanding"
      - "Pattern detection"
      - "Security analysis"
    temperature: 0.1
    max_tokens: 2000

  deepseek-coder:33b:
    description: "DeepSeek Coder 33B - Largest code model"
    size: "19 GB"
    parameters: "33B"
    use_case: "advanced_analysis"
    performance:
      speed: "slow"
      quality: "excellent"
      memory: "32GB RAM required"
    recommended_for:
      - "Complex contracts"
      - "Final audit"
      - "High-value contracts"
    temperature: 0.1
    max_tokens: 4000

  # General purpose models
  mistral:7b-instruct:
    description: "Mistral 7B Instruct - Fast, good explanations"
    size: "4.1 GB"
    parameters: "7B"
    use_case: "explanation_generation"
    performance:
      speed: "very_fast"
      quality: "good"
      memory: "8GB RAM recommended"
    recommended_for:
      - "Explaining findings"
      - "Documentation"
      - "User-friendly reports"
    temperature: 0.3
    max_tokens: 2000

  llama2:13b:
    description: "Meta's Llama 2 13B - General purpose"
    size: "7.4 GB"
    parameters: "13B"
    use_case: "general_purpose"
    performance:
      speed: "medium"
      quality: "good"
      memory: "16GB RAM recommended"
    recommended_for:
      - "General analysis"
      - "Backup model"
    temperature: 0.2
    max_tokens: 2000

  # Specialized models
  phi:latest:
    description: "Microsoft Phi - Tiny but capable"
    size: "1.6 GB"
    parameters: "2.7B"
    use_case: "resource_constrained"
    performance:
      speed: "very_fast"
      quality: "acceptable"
      memory: "4GB RAM sufficient"
    recommended_for:
      - "Low-resource environments"
      - "Quick checks"
      - "Rapid iteration"
    temperature: 0.1
    max_tokens: 1000

  starling-lm:7b:
    description: "Starling LM 7B - RLAIF trained"
    size: "4.1 GB"
    parameters: "7B"
    use_case: "high_quality_responses"
    performance:
      speed: "fast"
      quality: "very_high"
      memory: "8GB RAM recommended"
    recommended_for:
      - "Detailed analysis"
      - "Complex reasoning"
    temperature: 0.2
    max_tokens: 2000

# Recommended configurations by use case
recommendations:
  development:
    primary: "codellama:7b"
    fallback: "phi:latest"
    reason: "Fast feedback loop"

  ci_cd:
    primary: "deepseek-coder:6.7b"
    fallback: "codellama:7b"
    reason: "Balance of speed and quality"

  pre_audit:
    primary: "codellama:13b"
    fallback: "deepseek-coder:6.7b"
    reason: "High quality analysis"

  final_audit:
    primary: "deepseek-coder:33b"
    fallback: "codellama:13b"
    reason: "Maximum accuracy"

  explanation:
    primary: "mistral:7b-instruct"
    fallback: "llama2:13b"
    reason: "Clear, user-friendly output"

  resource_constrained:
    primary: "phi:latest"
    fallback: "codellama:7b"
    reason: "Minimal resource usage"

# Model comparison
comparison:
  speed_ranking:
    - "phi:latest"           # Fastest
    - "mistral:7b-instruct"
    - "codellama:7b"
    - "deepseek-coder:6.7b"
    - "starling-lm:7b"
    - "llama2:13b"
    - "codellama:13b"
    - "deepseek-coder:33b"   # Slowest

  quality_ranking:
    - "deepseek-coder:33b"   # Best
    - "codellama:13b"
    - "starling-lm:7b"
    - "deepseek-coder:6.7b"
    - "mistral:7b-instruct"
    - "codellama:7b"
    - "llama2:13b"
    - "phi:latest"           # Good but limited

  size_ranking:
    - "phi:latest"           # Smallest (1.6GB)
    - "codellama:7b"         # 3.8GB
    - "deepseek-coder:6.7b"  # 3.8GB
    - "mistral:7b-instruct"  # 4.1GB
    - "starling-lm:7b"       # 4.1GB
    - "codellama:13b"        # 7.4GB
    - "llama2:13b"           # 7.4GB
    - "deepseek-coder:33b"   # Largest (19GB)

# Installation commands
installation:
  all_recommended:
    - "ollama pull codellama:13b"
    - "ollama pull deepseek-coder:6.7b"
    - "ollama pull mistral:7b-instruct"

  minimal:
    - "ollama pull codellama:7b"

  complete:
    - "ollama pull codellama:7b"
    - "ollama pull codellama:13b"
    - "ollama pull deepseek-coder:6.7b"
    - "ollama pull deepseek-coder:33b"
    - "ollama pull mistral:7b-instruct"
    - "ollama pull phi:latest"

# Performance benchmarks (approximate, on M1 Mac)
benchmarks:
  codellama:13b:
    tokens_per_second: 15-20
    cold_start: "5-10s"
    analysis_time: "60-90s"

  codellama:7b:
    tokens_per_second: 25-35
    cold_start: "3-5s"
    analysis_time: "30-45s"

  deepseek-coder:6.7b:
    tokens_per_second: 30-40
    cold_start: "3-5s"
    analysis_time: "25-40s"

  phi:latest:
    tokens_per_second: 50-70
    cold_start: "1-2s"
    analysis_time: "15-25s"
