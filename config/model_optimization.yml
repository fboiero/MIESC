# MIESC - Model Optimization Configuration
#
# This file contains recommended model configurations for different scenarios
# and hardware specifications.

# =============================================================================
# BY USE CASE
# =============================================================================

use_cases:
  development:
    description: "Fast feedback during development"
    recommended_model: "codellama:7b"
    rationale: "Fastest model (30s), good enough for dev feedback"
    min_ram: "8GB"
    expected_time: "30s"
    quality_score: 7.0
    cost: "$0.00"

  cicd:
    description: "Automated CI/CD security checks"
    recommended_model: "deepseek-coder:6.7b"
    rationale: "Balanced speed/quality for automated checks"
    min_ram: "8GB"
    expected_time: "45s"
    quality_score: 7.8
    cost: "$0.00"

  pre_audit:
    description: "Comprehensive analysis before external audit"
    recommended_model: "codellama:13b"
    alternative_model: "deepseek-coder:33b"  # If time permits
    rationale: "High quality analysis, acceptable speed"
    min_ram: "12GB"
    expected_time: "60s"
    quality_score: 8.2
    cost: "$0.00"

  production:
    description: "Final audit for production deployment"
    recommended_model: "deepseek-coder:33b"
    rationale: "Best quality, worth the extra time"
    min_ram: "24GB"
    expected_time: "120s"
    quality_score: 8.9
    cost: "$0.00"

  quick_scan:
    description: "Quick vulnerability scan"
    recommended_model: "phi:latest"
    rationale: "Fastest possible, minimal resources"
    min_ram: "4GB"
    expected_time: "15s"
    quality_score: 6.5
    cost: "$0.00"

  defi_specific:
    description: "DeFi protocol analysis"
    recommended_model: "deepseek-coder:33b"
    rationale: "Best for complex DeFi logic"
    min_ram: "24GB"
    expected_time: "120s"
    quality_score: 9.0
    cost: "$0.00"
    custom_prompt: true  # Use DeFi-specific prompts

# =============================================================================
# BY HARDWARE SPECS
# =============================================================================

hardware_profiles:
  low_end:
    description: "Low-resource machine"
    min_specs:
      ram: "4-8GB"
      cpu_cores: "2-4"
      disk: "5GB"
    recommended_models:
      - model: "phi:latest"
        size: "1.6GB"
        expected_time: "15s"
        quality: "Good"
      - model: "codellama:7b"
        size: "3.8GB"
        expected_time: "30s"
        quality: "Good+"
    not_recommended:
      - "codellama:13b"  # Too slow
      - "deepseek-coder:33b"  # Won't fit in RAM

  mid_range:
    description: "Standard development machine"
    min_specs:
      ram: "8-16GB"
      cpu_cores: "4-8"
      disk: "15GB"
    recommended_models:
      - model: "codellama:7b"
        size: "3.8GB"
        expected_time: "20s"
        quality: "Good+"
      - model: "codellama:13b"
        size: "7.4GB"
        expected_time: "45s"
        quality: "High"
      - model: "deepseek-coder:6.7b"
        size: "3.8GB"
        expected_time: "35s"
        quality: "High"
    not_recommended:
      - "deepseek-coder:33b"  # May struggle

  high_end:
    description: "High-performance machine"
    min_specs:
      ram: "16GB+"
      cpu_cores: "8+"
      disk: "30GB"
    recommended_models:
      - model: "codellama:13b"
        size: "7.4GB"
        expected_time: "30s"
        quality: "High"
      - model: "deepseek-coder:33b"
        size: "19GB"
        expected_time: "60s"
        quality: "Excellent"
      - model: "mistral:7b-instruct"
        size: "4.1GB"
        expected_time: "25s"
        quality: "High"
    all_models_supported: true

  with_gpu:
    description: "Machine with GPU acceleration"
    min_specs:
      ram: "16GB+"
      gpu_vram: "8GB+"
      disk: "30GB"
    recommended_models:
      - model: "deepseek-coder:33b"
        size: "19GB"
        expected_time: "30s"  # Much faster with GPU
        quality: "Excellent"
      - model: "codellama:13b"
        size: "7.4GB"
        expected_time: "15s"
        quality: "High"
    notes: "GPU can 2-4x speed improvements"

# =============================================================================
# MODEL COMPARISON MATRIX
# =============================================================================

models:
  phi:latest:
    parameters: "2.7B"
    size: "1.6GB"
    min_ram: "4GB"
    speed: "Very Fast"
    quality: "Good"
    speed_score: 10
    quality_score: 6.5
    use_for:
      - "Quick scans"
      - "Low-resource environments"
      - "Initial triage"
    not_for:
      - "Production audits"
      - "Critical contracts"

  codellama:7b:
    parameters: "7B"
    size: "3.8GB"
    min_ram: "8GB"
    speed: "Fast"
    quality: "Good+"
    speed_score: 9
    quality_score: 7.5
    use_for:
      - "Development workflow"
      - "CI/CD pipelines"
      - "Frequent scans"
    benchmark:
      precision: 0.70
      recall: 0.73
      f1_score: 0.715

  deepseek-coder:6.7b:
    parameters: "6.7B"
    size: "3.8GB"
    min_ram: "8GB"
    speed: "Fast"
    quality: "High"
    speed_score: 8
    quality_score: 7.9
    use_for:
      - "CI/CD pipelines"
      - "Automated analysis"
      - "Code-specific tasks"
    benchmark:
      precision: 0.78
      recall: 0.76
      f1_score: 0.77

  mistral:7b-instruct:
    parameters: "7B"
    size: "4.1GB"
    min_ram: "8GB"
    speed: "Fast"
    quality: "High"
    speed_score: 8
    quality_score: 7.8
    use_for:
      - "Explanations"
      - "User-friendly output"
      - "Educational analysis"
    notes: "Best for clear, user-friendly explanations"

  codellama:13b:
    parameters: "13B"
    size: "7.4GB"
    min_ram: "12GB"
    speed: "Medium"
    quality: "High"
    speed_score: 6
    quality_score: 8.2
    use_for:
      - "Pre-audit analysis"
      - "Medium-value contracts"
      - "General purpose"
    benchmark:
      precision: 0.75
      recall: 0.75
      f1_score: 0.75
    recommended: true  # Default recommended model

  deepseek-coder:33b:
    parameters: "33B"
    size: "19GB"
    min_ram: "24GB"
    speed: "Slow"
    quality: "Excellent"
    speed_score: 3
    quality_score: 9.0
    use_for:
      - "Production audits"
      - "High-value contracts"
      - "DeFi protocols"
      - "Complex logic"
    benchmark:
      precision: 0.80
      recall: 0.78
      f1_score: 0.79

# =============================================================================
# PERFORMANCE OPTIMIZATION TIPS
# =============================================================================

optimization_tips:
  general:
    - "Use smaller models for development, larger for production"
    - "Enable GPU if available (2-4x speedup)"
    - "Consider parallel analysis of multiple contracts"
    - "Cache results to avoid re-analyzing unchanged contracts"
    - "Use quick models for triage, detailed models for deep analysis"

  memory:
    - "Close other applications during analysis"
    - "Use smaller batch sizes if analyzing multiple contracts"
    - "Consider swap space if RAM is limited"
    - "Monitor memory usage: ollama ps"

  speed:
    - "Use phi:latest or codellama:7b for fastest results"
    - "GPU acceleration can provide 2-4x speedup"
    - "Parallel analysis on multi-core CPUs"
    - "Pre-pull models to avoid download time"

  quality:
    - "Use deepseek-coder:33b for best quality"
    - "Combine multiple models for validation"
    - "Use CrewAI multi-agent for higher confidence"
    - "Custom prompts for specific vulnerability types"

# =============================================================================
# COST-BENEFIT ANALYSIS
# =============================================================================

cost_benefit:
  scenarios:
    all_local:
      description: "Use only local Ollama models"
      cost_per_analysis: "$0.00"
      cost_per_100: "$0.00"
      annual_cost: "$0.00"  # Based on 3000 analyses/year
      pros:
        - "Zero cost"
        - "Complete privacy"
        - "No rate limits"
        - "Works offline"
      cons:
        - "Requires hardware"
        - "Slightly lower quality than GPT-4"
        - "Slower than cloud APIs"

    hybrid_95:
      description: "95% Ollama, 5% GPT-4 for critical"
      cost_per_analysis: "$0.0015"  # Average
      cost_per_100: "$0.15"
      annual_cost: "$4.50"
      savings_vs_all_gpt4: "$85.50"
      savings_percentage: "95%"
      pros:
        - "95% cost reduction"
        - "Best quality for critical contracts"
        - "Flexible"
      cons:
        - "Still need API keys"
        - "Some cloud dependency"

    all_gpt4:
      description: "Use GPT-4 for all analysis"
      cost_per_analysis: "$0.03"
      cost_per_100: "$3.00"
      annual_cost: "$90.00"
      pros:
        - "Best quality"
        - "Cloud convenience"
        - "Faster than local"
      cons:
        - "Expensive"
        - "Privacy concerns"
        - "Rate limits"
        - "Requires internet"

# =============================================================================
# MIGRATION GUIDE
# =============================================================================

migration:
  from_gpt4:
    step_1:
      action: "Start with development workflow"
      replace: "GPT-4 in development"
      with: "Ollama codellama:7b"
      savings: "~30 calls/month × $0.03 = $0.90/month"

    step_2:
      action: "Move CI/CD to Ollama"
      replace: "GPT-4 in CI/CD"
      with: "Ollama deepseek-coder:6.7b"
      savings: "~100 calls/month × $0.03 = $3.00/month"

    step_3:
      action: "Use hybrid for production"
      replace: "All GPT-4"
      with: "Ollama (95%) + GPT-4 (5%)"
      savings: "~95% of remaining costs"

    total_savings: "~$85/year"

  from_gpt35:
    step_1:
      action: "Replace all GPT-3.5 with Ollama"
      replace: "GPT-3.5"
      with: "Ollama codellama:13b"  # Better quality
      savings: "100% of GPT-3.5 costs"
      bonus: "Better quality than GPT-3.5"

# =============================================================================
# TROUBLESHOOTING
# =============================================================================

troubleshooting:
  out_of_memory:
    symptoms:
      - "Ollama crashes"
      - "System freezes"
      - "Very slow performance"
    solutions:
      - "Use smaller model (phi, codellama:7b)"
      - "Close other applications"
      - "Add swap space"
      - "Upgrade RAM"

  slow_performance:
    symptoms:
      - "Analysis takes >2 minutes"
      - "High CPU usage"
    solutions:
      - "Use smaller model"
      - "Enable GPU if available"
      - "Close background apps"
      - "Check: ollama ps"

  poor_quality:
    symptoms:
      - "Missing obvious vulnerabilities"
      - "Too many false positives"
    solutions:
      - "Use larger model (codellama:13b, deepseek-coder:33b)"
      - "Try CrewAI multi-agent"
      - "Custom prompts for specific issues"
      - "Combine multiple models"

# =============================================================================
# RECOMMENDED CONFIGURATIONS
# =============================================================================

recommended_configs:
  beginner:
    description: "Just starting with MIESC"
    model: "codellama:7b"
    why: "Fast, small, easy to setup"
    command: "python main_ai.py contract.sol test --use-ollama --ollama-model codellama:7b"

  intermediate:
    description: "Regular user, moderate resources"
    model: "codellama:13b"
    why: "Best balance of speed and quality"
    command: "python main_ai.py contract.sol test --use-ollama --ollama-model codellama:13b"

  advanced:
    description: "Power user, high-end machine"
    models:
      - "deepseek-coder:33b"  # Single agent
      - "ollama/codellama:13b"  # CrewAI
    why: "Best quality, multi-agent validation"
    command: "python main_ai.py contract.sol test --use-ollama --use-crewai --ollama-model deepseek-coder:33b"

  professional:
    description: "Professional auditor"
    strategy: "Multi-model validation"
    models:
      - "codellama:13b"
      - "deepseek-coder:33b"
      - "CrewAI multi-agent"
    workflow:
      - "Run all models in parallel"
      - "Compare results"
      - "High confidence = all models agree"
      - "Low confidence = models disagree, manual review needed"
